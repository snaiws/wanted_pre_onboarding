{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 1) Tokenizer 생성하기**\n",
        "\n",
        "**1-1. `preprocessing()`**\n",
        "\n",
        "텍스트 전처리를 하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
        "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
        "- 조건 2: 토큰화는 white space 단위로 수행합니다.\n",
        "    \n",
        "    \n",
        "\n",
        "**1-2. `fit()`**\n",
        "\n",
        "어휘 사전을 구축하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
        "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n",
        "    \n",
        "\n",
        "**1-3. `transform()`**\n",
        "\n",
        "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
        "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
      ],
      "metadata": {
        "id": "B9cUGi0ZYW_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hpzeezJXWAQP"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    '''\n",
        "    문제 1-1.\n",
        "    '''\n",
        "    import re\n",
        "    for sentence in sequences:\n",
        "      result.append(re.sub(r\"[^a-zA-Zㄱ-힣0-9\\s]\",\"\",sentence.lower()).split())\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    '''\n",
        "    self.word_dict = {'oov': 0}\n",
        "    num=1\n",
        "    for i in self.preprocessing(sequences):\n",
        "      for j in i:\n",
        "        if j not in self.word_dict:\n",
        "          self.word_dict[j]=num\n",
        "          num+=1\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      '''\n",
        "      for sentence in tokens:\n",
        "          result.append(list(map(lambda x: self.word_dict[x] if x in self.word_dict else self.word_dict['oov'],sentence)))\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example='In the 1600s the Dutch East India Company employed hundreds of ships to trade gold, porcelain, spices, and silks around the globe. But running this massive operation wasn’t cheap. In order to fund their expensive voyages, the company turned to private citizens– individuals who could invest money to support the trip in exchange for a share of the ship’s profits. This practice allowed the company to afford even grander voyages, increasing profits for both themselves and their savvy investors.'.split('.')\n",
        "example.pop()\n",
        "for sentence in example:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXMdMSe3FIdX",
        "outputId": "13fdb677-546d-4dc7-83a3-33dc67a2c53c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the 1600s the Dutch East India Company employed hundreds of ships to trade gold, porcelain, spices, and silks around the globe\n",
            " But running this massive operation wasn’t cheap\n",
            " In order to fund their expensive voyages, the company turned to private citizens– individuals who could invest money to support the trip in exchange for a share of the ship’s profits\n",
            " This practice allowed the company to afford even grander voyages, increasing profits for both themselves and their savvy investors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok=Tokenizer()\n",
        "tok.preprocessing(example)"
      ],
      "metadata": {
        "id": "ZqBqONrVGgq-",
        "outputId": "ec9c6a5a-991c-4c85-d34e-3b69d72fb5f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['in',\n",
              "  'the',\n",
              "  '1600s',\n",
              "  'the',\n",
              "  'dutch',\n",
              "  'east',\n",
              "  'india',\n",
              "  'company',\n",
              "  'employed',\n",
              "  'hundreds',\n",
              "  'of',\n",
              "  'ships',\n",
              "  'to',\n",
              "  'trade',\n",
              "  'gold',\n",
              "  'porcelain',\n",
              "  'spices',\n",
              "  'and',\n",
              "  'silks',\n",
              "  'around',\n",
              "  'the',\n",
              "  'globe'],\n",
              " ['but', 'running', 'this', 'massive', 'operation', 'wasnt', 'cheap'],\n",
              " ['in',\n",
              "  'order',\n",
              "  'to',\n",
              "  'fund',\n",
              "  'their',\n",
              "  'expensive',\n",
              "  'voyages',\n",
              "  'the',\n",
              "  'company',\n",
              "  'turned',\n",
              "  'to',\n",
              "  'private',\n",
              "  'citizens',\n",
              "  'individuals',\n",
              "  'who',\n",
              "  'could',\n",
              "  'invest',\n",
              "  'money',\n",
              "  'to',\n",
              "  'support',\n",
              "  'the',\n",
              "  'trip',\n",
              "  'in',\n",
              "  'exchange',\n",
              "  'for',\n",
              "  'a',\n",
              "  'share',\n",
              "  'of',\n",
              "  'the',\n",
              "  'ships',\n",
              "  'profits'],\n",
              " ['this',\n",
              "  'practice',\n",
              "  'allowed',\n",
              "  'the',\n",
              "  'company',\n",
              "  'to',\n",
              "  'afford',\n",
              "  'even',\n",
              "  'grander',\n",
              "  'voyages',\n",
              "  'increasing',\n",
              "  'profits',\n",
              "  'for',\n",
              "  'both',\n",
              "  'themselves',\n",
              "  'and',\n",
              "  'their',\n",
              "  'savvy',\n",
              "  'investors']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.fit_transform(example)"
      ],
      "metadata": {
        "id": "MrKV-wsrIgGW",
        "outputId": "ef19ad04-34b8-4040-dbdc-1f7de8d6205c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 2, 20],\n",
              " [21, 22, 23, 24, 25, 26, 27],\n",
              " [1,\n",
              "  28,\n",
              "  12,\n",
              "  29,\n",
              "  30,\n",
              "  31,\n",
              "  32,\n",
              "  2,\n",
              "  7,\n",
              "  33,\n",
              "  12,\n",
              "  34,\n",
              "  35,\n",
              "  36,\n",
              "  37,\n",
              "  38,\n",
              "  39,\n",
              "  40,\n",
              "  12,\n",
              "  41,\n",
              "  2,\n",
              "  42,\n",
              "  1,\n",
              "  43,\n",
              "  44,\n",
              "  45,\n",
              "  46,\n",
              "  10,\n",
              "  2,\n",
              "  11,\n",
              "  47],\n",
              " [23, 48, 49, 2, 7, 12, 50, 51, 52, 32, 53, 47, 44, 54, 55, 17, 30, 56, 57]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.word_dict"
      ],
      "metadata": {
        "id": "uIPjiNPFI43r",
        "outputId": "e80b7eff-cb9a-4a31-d3cd-e4d29ca8c8ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'1600s': 3,\n",
              " 'a': 45,\n",
              " 'afford': 50,\n",
              " 'allowed': 49,\n",
              " 'and': 17,\n",
              " 'around': 19,\n",
              " 'both': 54,\n",
              " 'but': 21,\n",
              " 'cheap': 27,\n",
              " 'citizens': 35,\n",
              " 'company': 7,\n",
              " 'could': 38,\n",
              " 'dutch': 4,\n",
              " 'east': 5,\n",
              " 'employed': 8,\n",
              " 'even': 51,\n",
              " 'exchange': 43,\n",
              " 'expensive': 31,\n",
              " 'for': 44,\n",
              " 'fund': 29,\n",
              " 'globe': 20,\n",
              " 'gold': 14,\n",
              " 'grander': 52,\n",
              " 'hundreds': 9,\n",
              " 'in': 1,\n",
              " 'increasing': 53,\n",
              " 'india': 6,\n",
              " 'individuals': 36,\n",
              " 'invest': 39,\n",
              " 'investors': 57,\n",
              " 'massive': 24,\n",
              " 'money': 40,\n",
              " 'of': 10,\n",
              " 'oov': 0,\n",
              " 'operation': 25,\n",
              " 'order': 28,\n",
              " 'porcelain': 15,\n",
              " 'practice': 48,\n",
              " 'private': 34,\n",
              " 'profits': 47,\n",
              " 'running': 22,\n",
              " 'savvy': 56,\n",
              " 'share': 46,\n",
              " 'ships': 11,\n",
              " 'silks': 18,\n",
              " 'spices': 16,\n",
              " 'support': 41,\n",
              " 'the': 2,\n",
              " 'their': 30,\n",
              " 'themselves': 55,\n",
              " 'this': 23,\n",
              " 'to': 12,\n",
              " 'trade': 13,\n",
              " 'trip': 42,\n",
              " 'turned': 33,\n",
              " 'voyages': 32,\n",
              " 'wasnt': 26,\n",
              " 'who': 37}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok.transform(['I go to school.', 'I LIKE pizza!'])"
      ],
      "metadata": {
        "id": "ub0gGG4wJKOd",
        "outputId": "e060b389-1e0d-4483-bfd2-8396af18a610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 12, 0], [0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok2=Tokenizer()\n",
        "print(tok2.fit_transform(['I go to school.', 'I LIKE pizza!']))\n",
        "print(tok2.word_dict)"
      ],
      "metadata": {
        "id": "lSAZJr8gLaO9",
        "outputId": "ff12307e-abe3-4d99-805e-b6569d449546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3, 4], [1, 5, 6]]\n",
            "{'oov': 0, 'i': 1, 'go': 2, 'to': 3, 'school': 4, 'like': 5, 'pizza': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 2) TfidfVectorizer 생성하기**\n",
        "\n",
        "**2-1. `fit()`**\n",
        "\n",
        "입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: IDF 행렬은 list 형태입니다.\n",
        "    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n",
        "- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
        "    \n",
        "    $$\n",
        "    idf(d,t)=log_e(\\frac{n}{1+df(d,t)})\n",
        "    $$\n",
        "    \n",
        "    - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n",
        "    - $n$ : 입력된 전체 문장 개수\n",
        "- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
        "    \n",
        "    \n",
        "\n",
        "**2-2. `transform()`**\n",
        "\n",
        "입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output : nested list 형태입니다.\n",
        "    \n",
        "    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n",
        "    \n",
        "    |  | 토큰1 | 토큰2 | 토큰3 |\n",
        "    | --- | --- | --- | --- |\n",
        "    | 문장1 | tf-idf(1,1) | tf-idf(1,2) | tf-idf(1,3) |\n",
        "    | 문장2 | tf-idf(2,1) | tf-idf(2,2) | tf-idf(2,3) |\n",
        "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
        "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
        "- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "    \n",
        "    $$\n",
        "    tf-idf(d,t) = tf(d,t) \\times idf(d,t)\n",
        "    $$"
      ],
      "metadata": {
        "id": "-n0RIjC_Ybfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer()\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    '''\n",
        "    import numpy as np\n",
        "    self.idf=list(self.tokenizer.word_dict.values())\n",
        "\n",
        "    cnt = [[0 for x in self.idf] for x in tokenized]\n",
        "    for n,i in enumerate(tokenized):\n",
        "      for j in i:\n",
        "        cnt[n][j]+=1\n",
        "\n",
        "    n_doc=len(tokenized)\n",
        "    for i in range(len(self.idf)):\n",
        "      count=0\n",
        "      for j in cnt:\n",
        "        if j[self.idf[i]]:\n",
        "          count +=1\n",
        "      self.idf[i]=np.log(n_doc/(1+count))\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      self.tfidf_matrix = [[0 for x in self.idf] for x in tokenized]\n",
        "      for n,i in enumerate(tokenized):\n",
        "        for j in i:\n",
        "          self.tfidf_matrix[n][j]+=1\n",
        "      for i in range(len(self.tfidf_matrix)):\n",
        "        for j in range(len(self.tfidf_matrix[i])):\n",
        "          self.tfidf_matrix[i][j]=self.tfidf_matrix[i][j]*self.idf[j]\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "a--mjh87Xq1t"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=TfidfVectorizer(Tokenizer)\n",
        "tfidf.fit(example)\n",
        "tfidf.transform(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1euI3epbz56z",
        "outputId": "414f73b3-6d74-4cdd-ac26-4742961467eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.5753641449035617,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.28768207245178085,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453,\n",
              "  0.6931471805599453]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}