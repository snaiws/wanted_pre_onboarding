{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 1) Tokenizer 생성하기**\n",
        "\n",
        "**1-1. `preprocessing()`**\n",
        "\n",
        "텍스트 전처리를 하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장을 토큰화한 결과로, nested list 형태입니다. ex) [['i', 'go', 'to', 'school'], ['i', 'like', 'pizza']]\n",
        "- 조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
        "- 조건 2: 토큰화는 white space 단위로 수행합니다.\n",
        "    \n",
        "    \n",
        "\n",
        "**1-2. `fit()`**\n",
        "\n",
        "어휘 사전을 구축하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: 위에서 만든 `preprocessing` 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "- 조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
        "    - 주어진 코드에 있는 `self.word_dict`를 활용합니다.\n",
        "    \n",
        "\n",
        "**1-3. `transform()`**\n",
        "\n",
        "어휘 사전을 활용하여 입력 문장을 정수 인덱싱하는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output: 각 문장의 정수 인덱싱으로, nested list 형태입니다. ex) [[1, 2, 3, 4], [1, 5, 6]]\n",
        "- 조건 1: 어휘 사전(`self.word_dict`)에 없는 단어는 'oov'의 index로 변환합니다."
      ],
      "metadata": {
        "id": "B9cUGi0ZYW_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "hpzeezJXWAQP"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "    '''\n",
        "    문제 1-1.\n",
        "    '''\n",
        "    import re\n",
        "    for sentence in sequences:\n",
        "      result.append(re.sub(r\"[^a-z&\\s]\",\"\",sentence.lower()).split())\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    '''\n",
        "    문제 1-2.\n",
        "    '''\n",
        "    self.word_dict = {'oov': 0}\n",
        "    num=1\n",
        "    for i in self.preprocessing(sequences):\n",
        "      for j in i:\n",
        "        if j not in self.word_dict:\n",
        "          self.word_dict[j]=num\n",
        "          num+=1\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "      '''\n",
        "      문제 1-3.\n",
        "      '''\n",
        "      for sentence in tokens:\n",
        "          result.append(list(map(lambda x: self.word_dict[x] if x in self.word_dict else self.word_dict['oov'],sentence)))\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **문제 2) TfidfVectorizer 생성하기**\n",
        "\n",
        "**2-1. `fit()`**\n",
        "\n",
        "입력 문장들을 이용해 IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list 입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- 조건 1: IDF 행렬은 list 형태입니다.\n",
        "    - ex) [토큰1에 대한 IDF 값, 토큰2에 대한 IDF 값, .... ]\n",
        "- 조건 2: IDF 값은 아래 식을 이용해 구합니다.\n",
        "    \n",
        "    $$\n",
        "    idf(d,t)=log_e(\\frac{n}{1+df(d,t)})\n",
        "    $$\n",
        "    \n",
        "    - $df(d,t)$ : 단어 t가 포함된 문장 d의 개수\n",
        "    - $n$ : 입력된 전체 문장 개수\n",
        "- 조건 3: 입력된 문장의 토큰화에는 문제 1에서 만든 Tokenizer를 사용합니다.\n",
        "    \n",
        "    \n",
        "\n",
        "**2-2. `transform()`**\n",
        "\n",
        "입력 문장들을 이용해 TF-IDF 행렬을 만드는 함수입니다.\n",
        "\n",
        "- input: 여러 영어 문장이 포함된 list입니다. ex) ['I go to school.', 'I LIKE pizza!']\n",
        "- output : nested list 형태입니다.\n",
        "    \n",
        "    ex) [[tf-idf(1, 1), tf-idf(1, 2), tf-idf(1, 3)], [tf-idf(2, 1), tf-idf(2, 2), tf-idf(2, 3)]]\n",
        "    \n",
        "    |  | 토큰1 | 토큰2 | 토큰3 |\n",
        "    | --- | --- | --- | --- |\n",
        "    | 문장1 | tf-idf(1,1) | tf-idf(1,2) | tf-idf(1,3) |\n",
        "    | 문장2 | tf-idf(2,1) | tf-idf(2,2) | tf-idf(2,3) |\n",
        "- 조건1 : 입력 문장을 이용해 TF 행렬을 만드세요.\n",
        "    - $tf(d, t)$ : 문장 d에 단어 t가 나타난 횟수\n",
        "- 조건2 : 문제 2-1( `fit()`)에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "    \n",
        "    $$\n",
        "    tf-idf(d,t) = tf(d,t) \\times idf(d,t)\n",
        "    $$"
      ],
      "metadata": {
        "id": "-n0RIjC_Ybfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer()\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    '''\n",
        "    문제 2-1.\n",
        "    '''\n",
        "    import numpy as np\n",
        "    self.idf=list(self.tokenizer.word_dict.values())\n",
        "\n",
        "    cnt = [[0 for x in self.idf] for x in tokenized]\n",
        "    for n,i in enumerate(tokenized):\n",
        "      for j in i:\n",
        "        cnt[n][j]+=1\n",
        "\n",
        "    n_doc=len(tokenized)\n",
        "    for i in range(len(self.idf)):\n",
        "      count=0\n",
        "      for j in cnt:\n",
        "        if j[self.idf[i]]:\n",
        "          count +=1\n",
        "      self.idf[i]=np.log(n_doc/(1+count))\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      '''\n",
        "      문제 2-2.\n",
        "      '''\n",
        "      self.tfidf_matrix = [[0 for x in self.idf] for x in tokenized]\n",
        "      for n,i in enumerate(tokenized):\n",
        "        for j in i:\n",
        "          self.tfidf_matrix[n][j]+=1\n",
        "      for i in range(len(self.tfidf_matrix)):\n",
        "        for j in range(len(self.tfidf_matrix[i])):\n",
        "          self.tfidf_matrix[i][j]=self.tfidf_matrix[i][j]*self.idf[j]\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "a--mjh87Xq1t"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example='In the 1600s the Dutch East India Company employed hundreds of ships to trade gold, porcelain, spices, and silks around the globe. But running this massive operation wasn’t cheap. In order to fund their expensive voyages, the company turned to private citizens– individuals who could invest money to support the trip in exchange for a share of the ship’s profits. This practice allowed the company to afford even grander voyages, increasing profits for both themselves and their savvy investors.'.split('.')\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXMdMSe3FIdX",
        "outputId": "f3a23a8c-bf32-451d-e4d5-c9bd67d58433"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In the 1600s the Dutch East India Company employed hundreds of ships to trade gold, porcelain, spices, and silks around the globe', ' But running this massive operation wasn’t cheap', ' In order to fund their expensive voyages, the company turned to private citizens– individuals who could invest money to support the trip in exchange for a share of the ship’s profits', ' This practice allowed the company to afford even grander voyages, increasing profits for both themselves and their savvy investors', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=TfidfVectorizer(Tokenizer)\n",
        "tfidf.fit(example)\n",
        "tfidf.transform(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1euI3epbz56z",
        "outputId": "754923b3-eea1-4fdf-a50e-d0cddf61f000"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0,\n",
              "  0.5108256237659907,\n",
              "  0.6694306539426294,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.22314355131420976,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.5108256237659907,\n",
              "  0.22314355131420976,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  1.0216512475319814,\n",
              "  0.6694306539426294,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.22314355131420976,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.5108256237659907,\n",
              "  0.6694306539426294,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.5108256237659907,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.22314355131420976,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.22314355131420976,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.22314355131420976,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.5108256237659907,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551,\n",
              "  0.9162907318741551],\n",
              " [0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    }
  ]
}
